{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import argparse\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from scipy.special import softmax\n",
    "import evaluate\n",
    "from datasets import Dataset, features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the kaggle provided data\n",
    "* Loading jsons\n",
    "* Downsampling negative samples of the data\n",
    "* Defining and applying tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\n",
    "\n",
    "# convert csv to json and append to data\n",
    "# mistral_df = pd.read_csv(\"kaggle/input/pii-detection-removal-from-educational-data/100_gen_data.csv\")\n",
    "# mistral_df.head()\n",
    "# for i in range(len(mistral_df)):\n",
    "#     data.append({\"tokens\": mistral_df['tokenized response'][i], \"labels\": mistral_df['labels'][i]})\n",
    "\n",
    "mistral_data = json.load(open(\"kaggle/input/pii-detection-removal-from-educational-data/mistral_data.json\"))\n",
    "for d in mistral_data:\n",
    "    data.append(d)\n",
    "\n",
    "# # downsampling of negative examples\n",
    "# # p=[] # positive samples (contain relevant labels)\n",
    "# # n=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\n",
    "# # for d in data:\n",
    "# #     if any(np.array(d[\"labels\"]) != \"O\"): p.append(d)\n",
    "# #     else: n.append(d)\n",
    "print(\"original datapoints: \", len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "target = [\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n",
    "    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n",
    "    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n",
    "]\n",
    "\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, label2id, max_length):\n",
    "\n",
    "    # rebuild text from tokens\n",
    "    text = []\n",
    "    labels = []\n",
    "\n",
    "    for t, l, ws in zip(\n",
    "        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n",
    "    ):\n",
    "        text.append(t)\n",
    "        labels.extend([l] * len(t))\n",
    "\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "\n",
    "    # actual tokenization\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # CLS token\n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"])\n",
    "            continue\n",
    "\n",
    "        # case when token starts with whitespace\n",
    "        if text[start_idx].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        token_labels.append(label2id[labels[start_idx]])\n",
    "\n",
    "    length = len(tokenized.input_ids)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the data utilizing deberta tokenizer\n",
    "TRAIN_MODEL_PATH = \"microsoft/deberta-base\"\n",
    "TRAIN_MAX_LENGTH = 1024\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAIN_MODEL_PATH)\n",
    "\n",
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [str(x[\"document\"]) for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "    \"provided_labels\": [x[\"labels\"] for x in data],\n",
    "})\n",
    "\n",
    "ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": TRAIN_MAX_LENGTH}, num_proc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def downsample(df, percent):\n",
    "#     df = df.copy()\n",
    "\n",
    "#     df['is_labels'] = df['provided_labels'].apply(lambda labels: any(label != \"O\" for label in labels))\n",
    "#     true_samples = df[df['is_labels'] == True]\n",
    "#     false_samples = df[df['is_labels'] == False]\n",
    "\n",
    "#     downsampled_false_samples = false_samples.sample(frac=percent, random_state=42)\n",
    "\n",
    "\n",
    "#     return pd.concat([true_samples, downsampled_false_samples])\n",
    "\n",
    "# # Downsample the negative samples of the dataset\n",
    "# df_train = pd.DataFrame(ds)\n",
    "# df_train = downsample(df_train, 0.2)\n",
    "# df_train = df_train.drop(columns=['is_labels'])\n",
    "\n",
    "# ds = Dataset.from_pandas(df_train)\n",
    "# # Splitting the dataset into training and validation sets for performance evaluation\n",
    "# ds = ds.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation of model\n",
    "* Defining metrics (precision, recall, and f5-score)\n",
    "* Training model\n",
    "* Evaluating on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score\n",
    "\n",
    "def metrics(p, all_labels):\n",
    "    preds, labels = p\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "            [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(preds, labels)\n",
    "        ]\n",
    "    true_labels = [\n",
    "            [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(preds, labels)\n",
    "        ]\n",
    "\n",
    "    precision = precision_score(true_labels, true_predictions, average='micro')\n",
    "    recall = recall_score(true_labels, true_predictions, average='micro')\n",
    "\n",
    "    f5_score = (1 + 5 ** 2) * (precision * recall) / (5 ** 2 * precision + recall)\n",
    "\n",
    "    results = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f5\": f5_score\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    TRAIN_MODEL_PATH,\n",
    "    num_labels=len(all_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "# mps GPU acceleration for training \n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    model.to(mps_device)\n",
    "    print(\"Model moved to MPS device.\")\n",
    "elif torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    print(\"Model moved to CUDA device.\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU.\")\n",
    "\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.shuffle(seed=42).select(range(100))\n",
    "\n",
    "num_folds = 5\n",
    "fold_size = len(dataset) // num_folds\n",
    "\n",
    "folds = []\n",
    "for i in range(num_folds):\n",
    "    # Calculate start and end indices\n",
    "    start = i * fold_size\n",
    "    end = start + fold_size if i < num_folds - 1 else len(dataset)\n",
    "\n",
    "    # Select data for the fold\n",
    "    fold = dataset.select(range(start, end))\n",
    "    folds.append(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='kaggle/output', \n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    report_to=\"none\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    do_eval=False,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=20,\n",
    "    lr_scheduler_type='cosine',\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "dataset = ds.shuffle(seed=42)\n",
    "\n",
    "    # Define the number of folds\n",
    "num_folds = 5\n",
    "fold_size = len(dataset) // num_folds\n",
    "\n",
    "folds = []\n",
    "for i in range(num_folds):\n",
    "    # Calculate start and end indices\n",
    "    start = i * fold_size\n",
    "    end = start + fold_size if i < num_folds - 1 else len(dataset)\n",
    "\n",
    "    # Select data for the fold\n",
    "    fold = dataset.select(range(start, end))\n",
    "    folds.append(fold)\n",
    "# train a different model for 5 different folds of the dataset\n",
    "for i in range(num_folds):\n",
    "    print(f\"Training fold {i}\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        TRAIN_MODEL_PATH,\n",
    "        num_labels=len(all_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        mps_device = torch.device(\"mps\")\n",
    "        model.to(mps_device)\n",
    "        print(\"Model moved to MPS device.\")\n",
    "    elif torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "        print(\"Model moved to CUDA device.\")\n",
    "    else:\n",
    "        print(\"No GPU available, using CPU.\")\n",
    "\n",
    "    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=folds[i],\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=partial(metrics, all_labels=all_labels)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"deberta3base_1024_fold_{i}\")\n",
    "    tokenizer.save_pretrained(f\"deberta3base_1024_fold_{i}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "This is a slightly different procedure. We are implementing a weighted voting approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_tokenize(example, tokenizer, max_length):\n",
    "\n",
    "    # rebuild text from tokens\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "        idx += 1\n",
    "\n",
    "    # actual tokenization\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length)\n",
    "\n",
    "    return {**tokenized, \"token_map\": token_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\"deberta3base_1024_fold_0\": 2/10, \n",
    "               \"deberta3base_1024_fold_1\": 2/10, \n",
    "               \"deberta3base_1024_fold_2\": 2/10, \n",
    "               \"deberta3base_1024_fold_3\": 2/10, \n",
    "               \"deberta3base_1024_fold_4\": 2/10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble method prediction\n",
    "def ensemble_predict(model_paths, dataset):\n",
    "    all_preds = []\n",
    "\n",
    "    for path, weight in model_paths.items():\n",
    "        model = AutoModelForTokenClassification.from_pretrained(path)\n",
    "\n",
    "        # if torch.backends.mps.is_available():\n",
    "        #     mps_device = torch.device(\"mps\")\n",
    "        #     model.to(mps_device)\n",
    "        #     print(\"Model moved to MPS device.\")\n",
    "        # elif torch.cuda.is_available():\n",
    "        #     model.cuda()\n",
    "        #     print(\"Model moved to CUDA device.\")\n",
    "        # else:\n",
    "        #     print(\"No GPU available, using CPU.\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            \".\",\n",
    "            per_device_eval_batch_size=1,\n",
    "            report_to=\"none\",\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            args,\n",
    "            data_collator=collator,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        predictions = trainer.predict(dataset).predictions\n",
    "        weighted_predictions = softmax(predictions, axis=-1)\n",
    "        print(type(weighted_predictions))\n",
    "        print(weight)\n",
    "        weighted_predictions = weighted_predictions * weight\n",
    "        all_preds.append(weighted_predictions)\n",
    "\n",
    "    weighted_average_preds = np.sum(all_preds, axis=0) / sum(model_paths.values())\n",
    "\n",
    "    return weighted_average_preds\n",
    "\n",
    "\n",
    "def predict(model, dataset):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \".\",\n",
    "        per_device_eval_batch_size=1,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    predictions = trainer.predict(dataset).predictions\n",
    "    weighted_predictions = softmax(predictions, axis=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert test data\n",
    "test_data = json.load(open(\"kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n",
    "test_frame = pd.DataFrame(test_data)\n",
    "ds_test = Dataset.from_pandas(test_frame)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deberta3base_1024_fold_0\")\n",
    "ds_test = ds_test.map(inference_tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": 2048}, num_proc=3)\n",
    "\n",
    "model_path = \"deberta3base_1024_fold_0\"\n",
    "threshold = 0.99\n",
    "weighted_average_predictions = ensemble_predict(model_paths, ds_test)\n",
    "\n",
    "config = json.load(open(Path(model_path) / \"config.json\"))\n",
    "id2label = config[\"id2label\"]\n",
    "preds = weighted_average_predictions.argmax(-1)\n",
    "preds_without_O = weighted_average_predictions[:,:,:12].argmax(-1)\n",
    "O_preds = weighted_average_predictions[:,:,12]\n",
    "preds_final = np.where(O_preds < threshold, preds_without_O , preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = []\n",
    "document, token, label, token_str = [], [], [], []\n",
    "for p, token_map, offsets, tokens, doc in zip(preds_final, ds_test[\"token_map\"], ds_test[\"offset_mapping\"], ds_test[\"tokens\"], ds_test[\"document\"]):\n",
    "\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        label_pred = id2label[str(token_pred)]\n",
    "\n",
    "        if start_idx + end_idx == 0: continue\n",
    "\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "\n",
    "        # ignore \"\\n\\n\"\n",
    "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        if start_idx >= len(token_map): break\n",
    "\n",
    "        token_id = token_map[start_idx]\n",
    "\n",
    "        # ignore \"O\" predictions and whitespace preds\n",
    "        if label_pred != \"O\" and token_id != -1:\n",
    "            triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "            if triplet not in triplets:\n",
    "                document.append(doc)\n",
    "                token.append(token_id)\n",
    "                label.append(label_pred)\n",
    "                token_str.append(tokens[token_id])\n",
    "                triplets.append(triplet)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"document\": document,\n",
    "    \"token\": token,\n",
    "    \"label\": label,\n",
    "    \"token_str\": token_str\n",
    "})\n",
    "\n",
    "df[\"row_id\"] = list(range(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial, model_paths, labels, all_labels, test_data):\n",
    "    # Define the search space for the weights\n",
    "    weights = {}\n",
    "    for model_name in model_paths:\n",
    "        weights[model_name] = trial.suggest_float(model_name, 0.0, 1.0)\n",
    "\n",
    "    # Create the ensemble model using the weights\n",
    "    weighted_models = {model_name: weights[model_name] for model_name in model_paths}\n",
    "\n",
    "    # Generate predictions using the ensemble model\n",
    "    predictions = ensemble_predict(weighted_models, test_data)\n",
    "\n",
    "    # Evaluate the predictions using the evaluation metric\n",
    "    metric_value = metrics((predictions, labels), all_labels)['f5']\n",
    "\n",
    "    return metric_value\n",
    "\n",
    "# Create a study object and optimize the weights\n",
    "data_splits = ds.shuffle(seed=42).train_test_split(test_size=0.1, seed=42)\n",
    "test_data = data_splits[\"test\"]\n",
    "model_names = [\"deberta3base_1024_fold_0\", \"deberta3base_1024_fold_1\", \"deberta3base_1024_fold_2\", \"deberta3base_1024_fold_3\", \"deberta3base_1024_fold_4\"]\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, model_names, test_data['labels'], all_labels, test_data), n_trials=50)\n",
    "\n",
    "# Get the best weights and their corresponding metric value\n",
    "best_weights = study.best_params.items()\n",
    "best_metric = study.best_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# save best_weights and best_metric\n",
    "with open('best_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(best_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = ds.shuffle(seed=42).train_test_split(test_size=0.1, seed=42)\n",
    "test_data = data_splits[\"test\"]\n",
    "preds = ensemble_predict(model_paths, test_data)\n",
    "metrics((preds, test_data[\"labels\"]), all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random sample of 10% of the data in ds\n",
    "\n",
    "test_df = data_df.sample(frac=0.1)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "preds = ensemble_predict(model_paths, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
